{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T09:52:32.936565Z",
     "start_time": "2019-05-08T09:52:32.928540Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    for l in range(1,L):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Models:\n",
    "\n",
    "    def __init__(self,X_train , Y_train , X_test , Y_test ,FC_layer_dims\n",
    "                 ,parameters_shape, learning_rate, num_epochs=50 ,\n",
    "                 minibatch_size , print_cost = True):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.FC_layer_dims = FC_layer_dims;\n",
    "        self.gray_scale = len(X_train.shape) == 3\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.num_epochs = num_epochs\\\n",
    "        self.print_cost = print_cost\n",
    "        \n",
    "    def __create_placeholders(self,n_H , n_W , n_C , n_y):\n",
    "        X = tf.placeholder(shape=[None , n_H, n_W, n_C] , dtype=tf.float32 , name='X')\n",
    "        Y = tf.placeholder(shape=[None, n_y] , dtype=tf.float32 , name=\"Y\")\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "    def __initialize_learning_parameters(self,shapes):\n",
    "        tf.set_random_seed(1)\n",
    "        L = len(shapes)+1\n",
    "        parameters = {}\n",
    "        for l in range(1,L):\n",
    "        parameters[\"W\" + str(l)] = tf.get_variable(\"W\"+str(l) ,shapes[\"W\"+str(l)],initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "    def __initialize_non_learning_parameters(self,stride,window_size):\n",
    "      #create dict for non learnable parameters Like stride and max_pool Window size\n",
    "      non_learn_para = {}\n",
    "        for l in range(1,len(window_size)+1):\n",
    "            non_learn_para[\"Conv_s\"+str(l)] = stride[l-1][0]non_learn_para[\"Maxpool_s\"+str(l)] = stride[l-1][1]\n",
    "            non_learn_para[\"WS\"+str(l)] = window_size[l-1]\n",
    "    return non_learn_para\n",
    "\n",
    "    def __first_half(self,X , parameters , non_learn_para):\n",
    "        NLP = non_learn_para\n",
    "        L = len(parameters)\n",
    "        P = X\n",
    "        for l in range(1,L+1):\n",
    "\n",
    "            #Conv layer\n",
    "            s = NLP[\"Conv_s\"+str(l)] \n",
    "            Z = tf.nn.conv2d(P , parameters[\"W\"+str(l)], strides=[1,s,s,1] , padding='SAME')\n",
    "            #Relu\n",
    "            A = tf.nn.relu(Z)\n",
    "            #MaxPool\n",
    "            f = NLP[\"WS\"+str(l)]\n",
    "            s = NLP[\"Maxpool_s\"+str(l)]\n",
    "            P = tf.nn.max_pool(A , ksize=[1,f,f,1] , strides= [1,s,s,1] , padding='SAME')\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "    def __second_half(self,P , layer_dims):\n",
    "        L = len(layer_dims)\n",
    "        for l in range(1,L+1):\n",
    "            Z = tf.contrib.layers.fully_connected(P , layer_dims[l-1] , activation_fn=None)\n",
    "\n",
    "    return Z\n",
    "\n",
    "    def __forward_propagation(self,X , parameters,non_learn_para , FC_layer_dims):\n",
    "\n",
    "        #run_first_half\n",
    "        P = first_half(X,parameters , non_learn_para)\n",
    "\n",
    "        #Flatten\n",
    "        P_flatten = tf.contrib.layers.flatten(P)\n",
    "\n",
    "        #run_second_half\n",
    "        Z = second_half(P_flatten , FC_layer_dims)\n",
    "    return Z\n",
    "\n",
    "    def compute_cost(self,Z , Y):\n",
    "\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z , labels=Y))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def random_mini_batches(self,X, Y, mini_batch_size = 64, seed = 0):\n",
    "        \"\"\"\n",
    "        Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "        Returns:\n",
    "        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[0]                  # number of training examples\n",
    "        mini_batches = []\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[permutation,:,:,:]\n",
    "        shuffled_Y = Y[permutation,:]\n",
    "\n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "            mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "            mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model(self,X_train , Y_train , X_test , Y_test ,FC_layer_dims,parameters_shape, learning_rate=0.001 \n",
    "          , num_epochs=50 , minibatch_size=64 , print_cost = True):\n",
    "  \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    m , n_H0 , n_W0 , n_C0 = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "\n",
    "    #create list to store the cost at each epochs\n",
    "    costs = []\n",
    "    #create placeholders\n",
    "    X ,Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    #initialize all weights\n",
    "    parameters = initialize_learning_parameters(parameters_shape)\n",
    "\n",
    "    #\n",
    "    non_learn_para = initialize_non_learning_parameters(stride, window_size)\n",
    "\n",
    "    #Forward propogate to calculate Z of layer L in first half\n",
    "    Z = forward_propagation(X , parameters,non_learn_para , FC_layer_dims)\n",
    "\n",
    "    #calculate Cost using ZL and Y\n",
    "    cost = compute_cost(Z , Y)\n",
    "\n",
    "    #Minimize the cost using Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # initialize all the global variable here (W and b are global varibles)\n",
    "    init = tf.global_variables_initializer()\n",
    "    seed = 0\n",
    "    with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0\n",
    "        seed = seed+1\n",
    "\n",
    "        minibatches = random_mini_batches(X_train , Y_train , minibatch_size , seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "        mini_X , mini_Y = minibatch\n",
    "\n",
    "        _ , temp_cost = sess.run([optimizer , cost] , feed_dict = {X:mini_X , Y:mini_Y})\n",
    "\n",
    "        epoch_cost += temp_cost\n",
    "\n",
    "        if print_cost==True:\n",
    "        print (\"Cost after epoch %i: %f\" % (epoch+1, epoch_cost))\n",
    "        costs.append(epoch_cost)\n",
    "\n",
    "    plt.plot(range(1,num_epochs+1) , costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(Z,1), tf.argmax(Y,1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
