{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qk9bYVofkZl3","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:04:50.158262Z","start_time":"2019-05-24T14:04:50.112358Z"},"id":"I1PBHo1RjmUA","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import pickle\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDhfXiWOnQvo","colab_type":"code","colab":{}},"source":["\n","def load_DataModule():\n","  DataModule = drive.CreateFile({'id':'1OQiv4PxLTDKGUrsq15KF6VyHqyMq58mi'})\n","  DataModule.GetContentFile('DataUtilsModule.ipynb')\n","  %run DataUtilsModule.ipynb\n","\n","load_DataModule()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00ZR8MOmjipD","colab_type":"text"},"source":["# **TensorFlow Model**"]},{"cell_type":"markdown","metadata":{"id":"hW6GNL9UQcYT","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:04:50.364406Z","start_time":"2019-05-24T14:04:50.342992Z"},"id":"euW4gaJrjmUR","colab_type":"code","colab":{}},"source":["class TenserFlowModel(object):\n","    \n","    def load_data(self):\n","        raise NotImplementedError(\"Load_data() Need To be Implemented\")\n","    \n","    def create_placeholders(self):\n","        raise NotImplementedError(\"create_placeholders() Needs to be Implemented\")\n","        \n","    def initialize_parameters(self):\n","        raise NotImplementedError(\"initialize_parameters() Needs to be Implemented\")\n","        \n","    def create_feed_dict(self , input_batch , label_batch):\n","        raise NotImplementedError(\"create_feed_dict() Needs to be Implemented\")\n","        \n","    def add_forward_model(self , input_data):\n","        raise NotImplementedError(\"add_model() Needs to be Implemented\")\n","        \n","    def add_cost_op(self , predict):\n","        raise NotImplementedError(\"add_loss_op() Needs to be Implemented\")\n","    \n","    def run_epoch(self , sess , input_data , input_labels):\n","        raise NotImplementedError(\"run_epoch Needs to be Implemented\")\n","    \n","    def fit(self):\n","        raise NotImplementedError(\"fit() Needs to be Implemented\")\n","        \n","    def predict(self , sess , input_data , input_labels=None):\n","        raise NotImplementedError(\"predict() Needs to be Implemented\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:19:22.995021Z","start_time":"2019-05-24T14:19:22.862325Z"},"code_folding":[],"id":"i_jRklBKjmUa","colab_type":"code","colab":{}},"source":["class tensorFLowCnnModel(TenserFlowModel):\n","    def __init__(self):\n","        tf.set_random_seed(1)\n","        self.img_height = None\n","        self.img_width = None\n","        self.img_channel = None\n","        self.totalClasses = 5\n","        self.learning_parameter_shape = {}\n","        self.learning_parameters = {}\n","        self.non_learning_parameters = {}\n","        self.stride = None\n","        self.window_size = None\n","        self.gray_scale = False\n","        self.dropout = []\n","        self.fully_connented_dims = []\n","        self.learning_rate = 0.001\n","        self.num_epochs=10\n","        self.dataPath = '../data/working/'\n","        self.minibatch_size = 64\n","        self.m = None\n","        self.best_validation = 0.0\n","        self.checkpoint_save_path = '../data/CheckPoints/'\n","        self.last_improvement_iter = None\n","        self.improved_str = ''\n","#         self.last_saved_model = None\n","        tensorFLowCnnModel.graph_summary_number = 1\n","        #Funtion\n","        self.load_data()\n","        self.apply_one_hot()\n","        self.__set_shape_var_of_Eachlayers()\n","        \n","        \n","    def apply_one_hot(self):\n","        self.Y_train = tf.one_hot(self.Y_train , self.totalClasses)\n","        self.Y_test = tf.one_hot(self.Y_test , self.totalClasses)\n","        self.Y_dev = tf.one_hot(self.Y_dev , self.totalClasses)\n","        \n","        with tf.Session() as sess:\n","            self.Y_train , self.Y_test,self.Y_dev = sess.run([self.Y_train , self.Y_test , self.Y_dev]) \n","#         print(\"After OneHot\")\n","#         print(self.X_train.shape)\n","#         print(self.Y_train.shape)\n","\n","#         print(self.X_test.shape)\n","#         print(self.Y_test.shape)\n","\n","#         print(self.X_dev.shape)\n","#         print(self.Y_dev.shape)\n","            \n","    def load_data(self):\n","        #if Data is Already is stored\n","        if(utils.ifDataExits('X_Train' , self.dataPath)):\n","            self.X_train , self.X_test , self.X_dev , self.Y_train , self.Y_test , self.Y_dev \\\n","                                                                        = utils.load_saved_data()\n","#             print(self.X_train.shape)\n","#             print(self.Y_train.shape)\n","\n","#             print(self.X_test.shape)\n","#             print(self.Y_test.shape)\n","\n","#             print(self.X_dev.shape)\n","#             print(self.Y_dev.shape)\n","        else:\n","            self.X_train,self.X_test,self.Y_train,self.Y_test,self.X_dev,self.Y_dev  \\\n","                                    = utils.load_data(5 ,img_size=100 , batch_size=400) \n","    def __set_shape_var_of_Eachlayers(self):\n","        #For Learning Parameters\n","        filter_shape = [\n","                            [4,4,1,8] ,  #Layer 1\n","                            [4,4,8,16] , #Layer 2\n","                            [4,4,16,32] ,#Layer 3 \n","                            [4,4,32,32] ,#Layer 4\n","                            [4,4,32,64]  #Layer 5\n","                        ]\n","        if not self.gray_scale:\n","            filter_shape[0][2] = 3 \n","        \n","        for index,shape in enumerate(filter_shape):\n","            layer = index+1\n","            self.learning_parameter_shape[\"W\" + str(layer)] = shape\n","#         print(\"Shape :\" , self.learning_parameter_shape)\n","        #For Non-Learning Parameters\n","        \n","        #[for first maxpool layer,for second,for third , ...]\n","        self.window_size = [2,2,2,2,2]\n","        #[stride of conv layer , stride for max pool layer]\n","        self.stride = [[1,2] , [1,2] , [1,2] , [1,2] , [1,2]]\n","        self.dropout = [1 , 1 ,0.7 , 1 , 1]\n","        \n","        #Shape For FullyConnected Layer\n","        self.fully_connented_dims = [128 , 64 , 5] #for last Layer except sofmax Layer\n","        \n","    def __create_placeholders(self):\n","        X = tf.placeholder(dtype=tf.float32 , \n","                           shape=[None , self.img_height , self.img_width , self.img_channel] ,\n","                           name = 'input_values'\n","                          )\n","        Y = tf.placeholder(dtype=tf.float32 ,\n","                           shape=[None , self.totalClasses] ,\n","                           name=\"input_labels\"\n","                          )\n","        return X,Y\n","    \n","    def __initialize_learning_parameters(self):\n","        layers = len(self.learning_parameter_shape)\n","        for l in range(1, layers+1):\n","            self.learning_parameters[\"W\"+str(l)]= \\\n","                                    tf.get_variable(\"W\"+str(l) ,\n","                                                    self.learning_parameter_shape[\"W\"+str(l)] ,\n","                                                    initializer=tf.contrib.layers.xavier_initializer(seed=1)\n","                                                   )\n","#         print(\"Parameters\" , self.learning_parameters)\n","    def __initialize_non_learning_parameters(self):\n","        layers = len(self.stride)\n","        \n","        for l in range(1,layers + 1):\n","            self.non_learning_parameters[\"Conv_s\"+str(l)] = self.stride[l-1][0]\n","            self.non_learning_parameters[\"Maxpool_s\"+str(l)] = self.stride[l-1][1]\n","            self.non_learning_parameters[\"Window_size\"+str(l)] = self.window_size[l-1]\n","            self.non_learning_parameters[\"dropout\"+str(l)] = self.dropout[l-1]\n","            \n","        \n","    def __initialize_parameters(self):\n","        self.__set_shape_var_of_Eachlayers()\n","        \n","        self.__initialize_learning_parameters()\n","        self.__initialize_non_learning_parameters()\n","\n","        \n","    def __create_feed_dict(self , input_batch , label_batch):\n","        feed_dict = {}\n","        feed_dict[self.X] = input_batch\n","        feed_dict[self.Y] = label_batch\n","        \n","        return feed_dict\n","    \n","    def layersBeforeFlatten(self, X):\n","        LP = self.learning_parameters\n","        NLP = self.non_learning_parameters\n","        layers = len(LP)\n","        \n","        for l in range(1,layers+1):\n","            #Extract All parameters Needed\n","            conv_s = NLP[\"Conv_s\"+str(l)]\n","            f = NLP[\"Window_size\"+str(l)]\n","            pool_s = NLP[\"Maxpool_s\"+str(l)]\n","            curr_filter = LP[\"W\"+str(l)]\n","#             print(\"curr_filter\" , curr_filter)\n","                \n","            #CONV Layer\n","            s = conv_s\n","            Z = tf.nn.conv2d(X , \n","                             curr_filter , \n","                             strides=[1,s,s,1] , \n","                             padding='SAME' , \n","                             name=\"Conv_Layer_{}\".format(l))\n","            \n","            #Activation\n","            A = tf.nn.relu(Z , \n","                           name=\"Relu_{}\".format(l))\n","            \n","            #Maxpool\n","            s = pool_s\n","            features = tf.nn.max_pool(A , \n","                                      ksize=[1,f,f,1] , \n","                                      strides=[1,s,s,1] ,\n","                                      padding='SAME' , \n","                                      name=\"Max_pool_{}\".format(l))\n","            features = tf.nn.dropout(features ,\n","                                     rate=1-self.dropout[l-1])\n","            X = features\n","        return features\n","\n","    def LayersAfterFlatten(self , X):\n","        layers = len(self.fully_connented_dims)\n","        \n","        for l in range(1,layers+1):\n","            Z = tf.contrib.layers.fully_connected(X  , self.fully_connented_dims[l-1] , activation_fn=None)\n","        return Z\n","    \n","    def __add_forward_model(self , input_data):\n","        X = self.layersBeforeFlatten(input_data)\n","        \n","        X_flatten = tf.contrib.layers.flatten(X)\n","        \n","        Z = self.LayersAfterFlatten(X_flatten)\n","        \n","        return Z\n","    \n","    def add_cost_op(self , predict , Y):\n","        \n","        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict , labels=Y))\n","        return cost\n","    \n","    def get_random_mini_batch(self ,mini_batch_size = 64):\n","        mini_batches = []\n","        \n","        #About Random actually data is already Shuffled\n","\n","        num_compelete_minibatches = math.floor(self.m / mini_batch_size)\n","        for k in range(num_compelete_minibatches):\n","            mini_batch_X = self.X_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n","            mini_batch_Y = self.Y_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n","            mini_batch = (mini_batch_X, mini_batch_Y)\n","            mini_batches.append(mini_batch)\n","        \n","        if self.m%num_compelete_minibatches != 0:\n","            mini_batch_X = self.X_train[num_compelete_minibatches * mini_batch_size : self.m,:,:,:]\n","            mini_batch_Y = self.Y_train[num_compelete_minibatches * mini_batch_size : self.m,:]\n","            mini_batch = (mini_batch_X, mini_batch_Y)\n","            mini_batches.append(mini_batch)\n","\n","        return mini_batches\n","    \n","    def run_epoch(self , sess , input_data , input_label):\n","        epoch_cost = 0\n","        minibatches = self.get_random_mini_batch()\n","\n","        for minibatch in minibatches:\n","            mini_X , mini_Y = minibatch\n","\n","            _ , temp_cost = sess.run([self.optimizer , self.cost] , \n","                                     feed_dict = {self.X:mini_X , self.Y:mini_Y})\n","            epoch_cost += temp_cost\n","        \n","        return epoch_cost \n","    \n","    def saveModel(self , sess , curr_validation_acc , epoch):\n","        saver = tf.train.Saver()\n","        #Saving The Model\n","        if(curr_validation_acc > self.best_validation):\n","            self.best_validation = curr_validation_acc\n","\n","            self.last_improvement_iter = epoch\n","            \n","            save_path = \"{}{}-{}-{}%/{}%\".format(self.checkpoint_save_path ,\n","                                                  \"my-model\",\n","                                                  epoch ,\n","                                                  round(curr_validation_acc*100,2),\n","                                                  round(curr_validation_acc*100,3))\n","#             self.last_saved_model = save_path\n","            with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"w\") as handle:\n","                handle.write(save_path)\n","            \n","            saver.save(sess , save_path)\n","            \n","            self.improved_str = \"*\"\n","        else:\n","            self.improved_str = ''\n","        \n","    \n","    def fit(self):\n","        tf.reset_default_graph()\n","        \n","        self.m , self.img_height , self.img_width , self.img_channel = self.X_train.shape\n","        self.totalClasses = self.Y_train.shape[1]\n","        \n","        with tf.name_scope(\"initial_steps\"):\n","            self.X,self.Y = self.__create_placeholders()\n","            self.__initialize_parameters()\n","        \n","        with tf.name_scope(\"forward_train\"):\n","            Z = self.__add_forward_model(self.X)\n","        \n","        with tf.name_scope(\"cost_op\"):\n","            self.cost = self.add_cost_op(Z ,self.Y)\n","        \n","        with tf.name_scope(\"train\"):\n","            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n","        \n","        with tf.name_scope(\"accuracy\"):\n","            correct_prediction = tf.equal(tf.argmax(Z,1) , tf.argmax(self.Y,1))\n","            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\"))\n","            \n","        with tf.name_scope(\"summarie_Training\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_train = tf.summary.merge_all()\n","            \n","        with tf.name_scope(\"summarie_Dev\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_dev = tf.summary.merge_all()\n","            \n","        with tf.name_scope(\"summarie_Test\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_test = tf.summary.merge_all()\n","            \n","        writer  = tf.summary.FileWriter(\"../tensorboard/./graphs/{}\".format(tensorFLowCnnModel.graph_summary_number))\n","        tensorFLowCnnModel.graph_summary_number += 1\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            \n","            writer.add_graph(sess.graph)\n","            \n","            costs = []\n","            \n","            #Restore Model If Exixts\n","            try:\n","                print(\"{}last_model.txt\".format(self.checkpoint_save_path))\n","                with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"r\") as file:\n","                    last_saved_model = file.read()\n","                    \n","                saver = tf.train.Saver()\n","                saver.restore(sess , last_saved_model)\n","            except:\n","                print(\"No Parameters Saved Yet\")\n","            \n","            for epoch in range(self.num_epochs):\n","                epoch_cost  = self.run_epoch(sess , self.X_train , self.Y_train)\n","\n","                curr_validation_acc = sess.run(accuracy , {self.X:self.X_dev , self.Y:self.Y_dev}) \n","                \n","                \n","            \n","                runned_summary = sess.run(self.summary_train , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                runned_summary = sess.run(self.summary_dev , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                runned_summary = sess.run(self.summary_test , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                \n","                self.saveModel(sess , curr_validation_acc , epoch)\n","                \n","                print(\"Cost after Epoch %i: %f\" % (epoch + 1 , epoch_cost),\n","                      \"=> Training Acc : %f\" %(sess.run(accuracy , {self.X:self.X_train , self.Y:self.Y_train})),\n","                      \"=> Dev Acc : %f\" %(curr_validation_acc),\n","                      \"=> Test Acc : %f\" %(sess.run(accuracy , {self.X:self.X_test , self.Y:self.Y_test})),\n","                      self.improved_str\n","                     )\n","                \n","                \n","                costs.append(epoch_cost)\n","#                 writer.add_summary(self.summary)\n","        writer.close()\n","        plt.plot(range(1,self.num_epochs+1) , costs)\n","        plt.ylabel('cost')\n","        plt.xlabel('iteration')\n","        plt.show()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8DoJo3jFjoN_","colab_type":"text"},"source":["# **Keras Model**"]},{"cell_type":"code","metadata":{"id":"Uh3BC6UGjmUk","colab_type":"code","colab":{}},"source":["class kerasModel():\n","\n","  def create_model():\n","    raise NotImplementedError(\"create_model is not Implemented Yet!\")\n","    \n","  def compile_model():\n","    raise NotImplementedError(\"compile_model is not Implemeted Yet!\")\n","    \n","  def run_model():\n","    raise NotImplementedError(\"run_model is not Implemented Yet!\")\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H9xry1C-mYmc","colab_type":"code","colab":{}},"source":["class KerasCnn(kerasModel):\n","  def __createConvLayers():\n","    pass\n"," \n","  def __createDenseLayers():\n","    pass\n","  \n","  def create_callbacks():\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5HXA108gKFD","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","#Callback Class 1\n","class onBeginCallback(tf.keras.callbacks.Callback):\n","  #logs contains batch_num and batch_size\n","  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n","    self.on_trainBegin = on_train\n","    self.on_testBegin = on_test\n","    self.on_epochBegin = on_epoch\n","    \n","  def on_train_batch_begin(self,batch , logs=None):\n","    if(self.on_trainBegin):\n","      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_test_batch_begin(self,batch , logs=None):\n","    if(self.on_testBegin):\n","      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n","  \n","  def on_epoch_begin(self, epoch, logs=None):\n","    if(self.on_epochBegin):\n","      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n","  \n","\n","#Callback Class 2\n","class onEndCallback(tf.keras.callbacks.Callback):\n","  #logs contains batch_num , batch_size , loss and Matric\n","  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n","    self.on_trainEnd = on_train\n","    self.on_testEnd = on_test\n","    self.on_epochEnd = on_epoch\n","  def on_train_batch_end(self,batch , logs=None):\n","    if(self.on_trainEnd):\n","      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_test_batch_end(self,batch , logs=None):\n","    if(self.on_testEnd):\n","      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_epoch_end(self, epoch, logs=None):\n","    if(self.on_epochEnd):\n","      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WT3CWtO6tFQR","colab_type":"code","colab":{}},"source":["class kerasCnnModel(KerasCnn):\n","  def __init__(self, parametersDict , callbackDict):\n","    self.callbackDict = callbackDict\n","    self.parametersDict = parametersDict\n","    self.filters = parametersDict['filters']\n","    self.Conv_layers = parametersDict['layers'][0]\n","    self.Dense_layers = parametersDict['layers'][1]\n","    self.kernel_size = parametersDict['kernel_size']\n","    self.padding = parametersDict['padding']\n","    self.dense_layers_size = parametersDict['dense_layers_size']\n","    self.activation = parametersDict['activation']\n","    self.pool_size = parametersDict['pool_size']\n","    self.dropout = parametersDict['dropout']\n","#     self.Convdrop = parametersDict['Convdrop']\n","#     self.flattenDrop = parametersDict['flattenDrop']\n","    random.seed(1)\n","    \n","    ###RUN INITIAL FUNTIONS\n","    self.__create_callbacks()\n","    \n","  def __createConvLayers(self):\n","    for i in range(self.Conv_layers):\n","      if i != 0:\n","        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , padding = self.padding[i][0]))\n","#         print(\"Conv2D(filters = {}, kernel_size = ({},{}) , padding = {})\".format(self.filters[i] ,self.kernel_size[i] , self.kernel_size[i] , self.padding[i][0]))\n","      else:\n","        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , input_shape = self.input_shape))\n","#         print(\"Conv2D(filters = {}, kernel_size = ({},{}) , padding = {})\".format(self.filters[i] ,self.kernel_size[i] , self.kernel_size[i] , self.padding[i][0]))\n","\n","      self.model.add(Activation(self.activation[i]))\n","#       print(\"Activation({})\".format(self.activation[i]))\n","      self.model.add(MaxPooling2D(pool_size = (self.pool_size[i] , self.pool_size[i]) , padding = self.padding[i][1]))\n","#       self.model.add(Dropout(self.Convdrop[i]))\n","  \n","#       print(\"MaxPooling2D(pool_size = ({} , {}) , padding = {})\".format(self.pool_size[i] ,self.pool_size[i] ,self.padding[i][1] ) )\n","#       print(\"\")\n","      \n","      \n","  def __createDenseLayers(self):\n","    for i in range(self.Dense_layers):\n","      self.model.add(Dense(self.dense_layers_size[i]))\n","#       print(\"(Dense({})\".format(self.dense_layers_size[i]))\n","      self.model.add(Activation(self.activation[self.Conv_layers + i]))\n","#       print(\"Activation({})\".format((self.activation[self.Conv_layers + i] ) ) ) \n","      if self.dropout!=False and i != (self.Dense_layers - 1):\n","        self.model.add(Dropout(self.dropout[i]))\n","#         print(\"Dropout({})\".format(self.dropout[i]))\n","      \n","      \n","  \n","  def create_model(self , input_shape=None , saved_model = None):\n","    self.input_shape = input_shape\n","    if saved_model == None:      \n","      self.model = Sequential()\n","      self.__createConvLayers()\n","      self.model.add(Flatten())\n","#       if self.flattenDrop:\n","#         self.model.add(Dropout(self.flattenDrop))\n","#         print(\"Adding\")\n","#       print(\"Not Adding\")\n","      self.__createDenseLayers()\n","      return self.model\n","    else:\n","      self.model = saved_model\n","    \n","    \n","  \n","  \n","  def compile_model(self , loss=\"sparse_categorical_crossentropy\"  , optimizer=tf.train.AdamOptimizer(0.001) , metrics=['accuracy']):\n","    self.model.compile(loss = loss,\n","                       optimizer = optimizer,\n","                       metrics = metrics\n","                      )\n","  @classmethod\n","  def getTotalSubfile(self,path):\n","    totalFiles = 0\n","    for dir_name in os.listdir(path):\n","      totalFiles += len(os.listdir(os.path.join(path , dir_name)))\n","    return totalFiles\n","  def run_model(self , path_to_dir ,  epochs=5 , batch_size=64 ):\n","    \n","    training_datagen = ImageDataGenerator(rescale= 1./255 , featurewise_center = False , featurewise_std_normalization=False)\n","    test_datagen = ImageDataGenerator(rescale= 1.255 , featurewise_center = False , featurewise_std_normalization=False)\n","    validation_datagen = ImageDataGenerator(rescale= 1./255 , featurewise_center = False , featurewise_std_normalization=False)\n","    \n","    training_dir = os.path.join(path_to_dir , 'Training')\n","    Test_dir = os.path.join(path_to_dir , 'Test')\n","    validation_dir = os.path.join(path_to_dir , 'Dev')\n","    \n","    \n","    training_generator = training_datagen.flow_from_directory(training_dir\n","                                             ,target_size=(256, 256)\n","                                             ,color_mode='rgb'\n","                                             ,class_mode='sparse'\n","                                             ,batch_size=batch_size\n","                                             ,shuffle=True\n","                                             ,seed = 1\n","                                            )\n","    \n","    validation_datagen = validation_datagen.flow_from_directory(\n","                                                        directory=validation_dir,\n","                                                        target_size=(256, 256),\n","                                                        color_mode=\"rgb\",\n","                                                        batch_size=batch_size,\n","                                                        class_mode=\"sparse\",\n","                                                        shuffle=True,\n","                                                        seed=1\n","                                                    )\n","    \n","    test_generator = test_datagen.flow_from_directory(\n","                                                      directory=Test_dir,\n","                                                      target_size=(256, 256),\n","                                                      color_mode=\"rgb\",\n","                                                      batch_size=batch_size,\n","                                                      class_mode=\"sparse\",\n","                                                      shuffle=True,\n","                                                      seed=1\n","                                                      )\n","    \n","    steps_per_epoch = math.ceil(self.getTotalSubfile(os.path.join(path_to_dir , \"Training\"))  / batch_size)\n","    step_size_validation = math.ceil(self.getTotalSubfile(os.path.join(path_to_dir , \"Dev\"))  / batch_size)\n","    print(steps_per_epoch)\n","    print(step_size_validation)\n","    \n","    \n","    history_callback = self.model.fit_generator(generator=training_generator\n","                                               ,steps_per_epoch=steps_per_epoch\n","                                               ,validation_data=validation_datagen\n","                                               ,validation_steps=step_size_validation\n","                                               ,epochs=epochs\n","                                               ,verbose=1\n","                                               ,callbacks=self.callbacks\n","                                               )\n","    \n","#     history_callback = self.model.fit_generator(image_gen.flow(X , Y , batch_size=batch_size) , steps_per_epoch=len(X)/batch_size , epoch = epochs)\n","#     for X_batch , Y_batch in image_gen.flow(X , Y , batch_size=batch_size):\n","#     history_callback = self.model.fit( \n","#                                        X \n","#                                       ,Y\n","#                                       ,batch_size=batch_size\n","#                                       ,epochs=epochs\n","#                                       ,validation_data = (X_dev , Y_dev)\n","#                                       ,callbacks=self.callbacks\n","#                                     )\n","  \n","  \n","  def evaluate_model(self , x_test , y_test):\n","    self.model.evaluate(x_test , y_test)\n","    \n","    \n","  def model_summary(self):\n","    print(self.model.summary())\n","    \n","  def load_model(self,path):\n","    return load_model(path)\n","    \n","  def __create_tensorboard_callback(self):\n","    return TensorBoard(log_dir=self.log_dir)\n","  \n","  \n","  def __create_ModelCheckpoint(self):\n","    i = 1\n","    while(True):\n","      modified_path = os.path.join(self.callbackDict['modelcheckpoint_path'] , 'model' + str(i) ) \n","      if os.path.exists(modified_path):\n","        i += 1\n","        continue\n","      else:\n","        os.mkdir(modified_path)\n","        self.callbackDict['modelcheckpoint_path']  = modified_path\n","        break\n","    path = os.path.join(self.callbackDict['modelcheckpoint_path'] , 'model.{epoch:02d}-{val_acc:.2f}.hdf5')\n","    return ModelCheckpoint(filepath=path\n","                          ,monitor= 'val_acc'\n","                          ,verbose= 1\n","                          ,save_best_only=True\n","                          ,mode='max'\n","                          ,save_weights_only=False,\n","                           save_freq='epoch'\n","                          )\n","  \n","  \n","  def __create_callbacks(self , log_dir=\"/content/gdrive/My Drive/SeedApp/data/logs\"):\n","    self.log_dir = log_dir\n","    self.callbacks = []\n","    \n","    if(self.callbackDict['custom']):\n","      self.callbacks.append(onBeginCallback())\n","      self.callbacks.append(onEndCallback())\n","      \n","    if(self.callbackDict['tensorboard']):\n","      cb = self.__create_tensorboard_callback()\n","      self.callbacks.append(cb)\n","      \n","    if(self.callbackDict['modelcheckpoint']):\n","      cb = self.__create_ModelCheckpoint()\n","      self.callbacks.append(cb)\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t23PFeo4BPIA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKL2bEeuab5O","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"Qm2-sY5EtC5S","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}