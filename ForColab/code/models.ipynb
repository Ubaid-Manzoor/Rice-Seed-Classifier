{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qk9bYVofkZl3","colab_type":"code","outputId":"bb115cd8-04f6-46c5-e7e2-b55a30cd3cf5","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▎                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 6.5MB/s \n","\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:04:50.158262Z","start_time":"2019-05-24T14:04:50.112358Z"},"id":"I1PBHo1RjmUA","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import pickle\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDhfXiWOnQvo","colab_type":"code","colab":{}},"source":["\n","def load_DataModule():\n","  DataModule = drive.CreateFile({'id':'1OQiv4PxLTDKGUrsq15KF6VyHqyMq58mi'})\n","  DataModule.GetContentFile('DataUtilsModule.ipynb')\n","  %run DataUtilsModule.ipynb\n","\n","load_DataModule()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00ZR8MOmjipD","colab_type":"text"},"source":["# **TensorFlow Model**"]},{"cell_type":"markdown","metadata":{"id":"hW6GNL9UQcYT","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:04:50.364406Z","start_time":"2019-05-24T14:04:50.342992Z"},"id":"euW4gaJrjmUR","colab_type":"code","colab":{}},"source":["class TenserFlowModel(object):\n","    \n","    def load_data(self):\n","        raise NotImplementedError(\"Load_data() Need To be Implemented\")\n","    \n","    def create_placeholders(self):\n","        raise NotImplementedError(\"create_placeholders() Needs to be Implemented\")\n","        \n","    def initialize_parameters(self):\n","        raise NotImplementedError(\"initialize_parameters() Needs to be Implemented\")\n","        \n","    def create_feed_dict(self , input_batch , label_batch):\n","        raise NotImplementedError(\"create_feed_dict() Needs to be Implemented\")\n","        \n","    def add_forward_model(self , input_data):\n","        raise NotImplementedError(\"add_model() Needs to be Implemented\")\n","        \n","    def add_cost_op(self , predict):\n","        raise NotImplementedError(\"add_loss_op() Needs to be Implemented\")\n","    \n","    def run_epoch(self , sess , input_data , input_labels):\n","        raise NotImplementedError(\"run_epoch Needs to be Implemented\")\n","    \n","    def fit(self):\n","        raise NotImplementedError(\"fit() Needs to be Implemented\")\n","        \n","    def predict(self , sess , input_data , input_labels=None):\n","        raise NotImplementedError(\"predict() Needs to be Implemented\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-05-24T14:19:22.995021Z","start_time":"2019-05-24T14:19:22.862325Z"},"code_folding":[],"id":"i_jRklBKjmUa","colab_type":"code","colab":{}},"source":["class tensorFLowCnnModel(TenserFlowModel):\n","    def __init__(self):\n","        tf.set_random_seed(1)\n","        self.img_height = None\n","        self.img_width = None\n","        self.img_channel = None\n","        self.totalClasses = 5\n","        self.learning_parameter_shape = {}\n","        self.learning_parameters = {}\n","        self.non_learning_parameters = {}\n","        self.stride = None\n","        self.window_size = None\n","        self.gray_scale = False\n","        self.dropout = []\n","        self.fully_connented_dims = []\n","        self.learning_rate = 0.001\n","        self.num_epochs=10\n","        self.dataPath = '../data/working/'\n","        self.minibatch_size = 64\n","        self.m = None\n","        self.best_validation = 0.0\n","        self.checkpoint_save_path = '../data/CheckPoints/'\n","        self.last_improvement_iter = None\n","        self.improved_str = ''\n","#         self.last_saved_model = None\n","        tensorFLowCnnModel.graph_summary_number = 1\n","        #Funtion\n","        self.load_data()\n","        self.apply_one_hot()\n","        self.__set_shape_var_of_Eachlayers()\n","        \n","        \n","    def apply_one_hot(self):\n","        self.Y_train = tf.one_hot(self.Y_train , self.totalClasses)\n","        self.Y_test = tf.one_hot(self.Y_test , self.totalClasses)\n","        self.Y_dev = tf.one_hot(self.Y_dev , self.totalClasses)\n","        \n","        with tf.Session() as sess:\n","            self.Y_train , self.Y_test,self.Y_dev = sess.run([self.Y_train , self.Y_test , self.Y_dev]) \n","#         print(\"After OneHot\")\n","#         print(self.X_train.shape)\n","#         print(self.Y_train.shape)\n","\n","#         print(self.X_test.shape)\n","#         print(self.Y_test.shape)\n","\n","#         print(self.X_dev.shape)\n","#         print(self.Y_dev.shape)\n","            \n","    def load_data(self):\n","        #if Data is Already is stored\n","        if(utils.ifDataExits('X_Train' , self.dataPath)):\n","            self.X_train , self.X_test , self.X_dev , self.Y_train , self.Y_test , self.Y_dev \\\n","                                                                        = utils.load_saved_data()\n","#             print(self.X_train.shape)\n","#             print(self.Y_train.shape)\n","\n","#             print(self.X_test.shape)\n","#             print(self.Y_test.shape)\n","\n","#             print(self.X_dev.shape)\n","#             print(self.Y_dev.shape)\n","        else:\n","            self.X_train,self.X_test,self.Y_train,self.Y_test,self.X_dev,self.Y_dev  \\\n","                                    = utils.load_data(5 ,img_size=100 , batch_size=400) \n","    def __set_shape_var_of_Eachlayers(self):\n","        #For Learning Parameters\n","        filter_shape = [\n","                            [4,4,1,8] ,  #Layer 1\n","                            [4,4,8,16] , #Layer 2\n","                            [4,4,16,32] ,#Layer 3 \n","                            [4,4,32,32] ,#Layer 4\n","                            [4,4,32,64]  #Layer 5\n","                        ]\n","        if not self.gray_scale:\n","            filter_shape[0][2] = 3 \n","        \n","        for index,shape in enumerate(filter_shape):\n","            layer = index+1\n","            self.learning_parameter_shape[\"W\" + str(layer)] = shape\n","#         print(\"Shape :\" , self.learning_parameter_shape)\n","        #For Non-Learning Parameters\n","        \n","        #[for first maxpool layer,for second,for third , ...]\n","        self.window_size = [2,2,2,2,2]\n","        #[stride of conv layer , stride for max pool layer]\n","        self.stride = [[1,2] , [1,2] , [1,2] , [1,2] , [1,2]]\n","        self.dropout = [1 , 1 ,0.7 , 1 , 1]\n","        \n","        #Shape For FullyConnected Layer\n","        self.fully_connented_dims = [128 , 64 , 5] #for last Layer except sofmax Layer\n","        \n","    def __create_placeholders(self):\n","        X = tf.placeholder(dtype=tf.float32 , \n","                           shape=[None , self.img_height , self.img_width , self.img_channel] ,\n","                           name = 'input_values'\n","                          )\n","        Y = tf.placeholder(dtype=tf.float32 ,\n","                           shape=[None , self.totalClasses] ,\n","                           name=\"input_labels\"\n","                          )\n","        return X,Y\n","    \n","    def __initialize_learning_parameters(self):\n","        layers = len(self.learning_parameter_shape)\n","        for l in range(1, layers+1):\n","            self.learning_parameters[\"W\"+str(l)]= \\\n","                                    tf.get_variable(\"W\"+str(l) ,\n","                                                    self.learning_parameter_shape[\"W\"+str(l)] ,\n","                                                    initializer=tf.contrib.layers.xavier_initializer(seed=1)\n","                                                   )\n","#         print(\"Parameters\" , self.learning_parameters)\n","    def __initialize_non_learning_parameters(self):\n","        layers = len(self.stride)\n","        \n","        for l in range(1,layers + 1):\n","            self.non_learning_parameters[\"Conv_s\"+str(l)] = self.stride[l-1][0]\n","            self.non_learning_parameters[\"Maxpool_s\"+str(l)] = self.stride[l-1][1]\n","            self.non_learning_parameters[\"Window_size\"+str(l)] = self.window_size[l-1]\n","            self.non_learning_parameters[\"dropout\"+str(l)] = self.dropout[l-1]\n","            \n","        \n","    def __initialize_parameters(self):\n","        self.__set_shape_var_of_Eachlayers()\n","        \n","        self.__initialize_learning_parameters()\n","        self.__initialize_non_learning_parameters()\n","\n","        \n","    def __create_feed_dict(self , input_batch , label_batch):\n","        feed_dict = {}\n","        feed_dict[self.X] = input_batch\n","        feed_dict[self.Y] = label_batch\n","        \n","        return feed_dict\n","    \n","    def layersBeforeFlatten(self, X):\n","        LP = self.learning_parameters\n","        NLP = self.non_learning_parameters\n","        layers = len(LP)\n","        \n","        for l in range(1,layers+1):\n","            #Extract All parameters Needed\n","            conv_s = NLP[\"Conv_s\"+str(l)]\n","            f = NLP[\"Window_size\"+str(l)]\n","            pool_s = NLP[\"Maxpool_s\"+str(l)]\n","            curr_filter = LP[\"W\"+str(l)]\n","#             print(\"curr_filter\" , curr_filter)\n","                \n","            #CONV Layer\n","            s = conv_s\n","            Z = tf.nn.conv2d(X , \n","                             curr_filter , \n","                             strides=[1,s,s,1] , \n","                             padding='SAME' , \n","                             name=\"Conv_Layer_{}\".format(l))\n","            \n","            #Activation\n","            A = tf.nn.relu(Z , \n","                           name=\"Relu_{}\".format(l))\n","            \n","            #Maxpool\n","            s = pool_s\n","            features = tf.nn.max_pool(A , \n","                                      ksize=[1,f,f,1] , \n","                                      strides=[1,s,s,1] ,\n","                                      padding='SAME' , \n","                                      name=\"Max_pool_{}\".format(l))\n","            features = tf.nn.dropout(features ,\n","                                     rate=1-self.dropout[l-1])\n","            X = features\n","        return features\n","\n","    def LayersAfterFlatten(self , X):\n","        layers = len(self.fully_connented_dims)\n","        \n","        for l in range(1,layers+1):\n","            Z = tf.contrib.layers.fully_connected(X  , self.fully_connented_dims[l-1] , activation_fn=None)\n","        return Z\n","    \n","    def __add_forward_model(self , input_data):\n","        X = self.layersBeforeFlatten(input_data)\n","        \n","        X_flatten = tf.contrib.layers.flatten(X)\n","        \n","        Z = self.LayersAfterFlatten(X_flatten)\n","        \n","        return Z\n","    \n","    def add_cost_op(self , predict , Y):\n","        \n","        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict , labels=Y))\n","        return cost\n","    \n","    def get_random_mini_batch(self ,mini_batch_size = 64):\n","        mini_batches = []\n","        \n","        #About Random actually data is already Shuffled\n","\n","        num_compelete_minibatches = math.floor(self.m / mini_batch_size)\n","        for k in range(num_compelete_minibatches):\n","            mini_batch_X = self.X_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n","            mini_batch_Y = self.Y_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n","            mini_batch = (mini_batch_X, mini_batch_Y)\n","            mini_batches.append(mini_batch)\n","        \n","        if self.m%num_compelete_minibatches != 0:\n","            mini_batch_X = self.X_train[num_compelete_minibatches * mini_batch_size : self.m,:,:,:]\n","            mini_batch_Y = self.Y_train[num_compelete_minibatches * mini_batch_size : self.m,:]\n","            mini_batch = (mini_batch_X, mini_batch_Y)\n","            mini_batches.append(mini_batch)\n","\n","        return mini_batches\n","    \n","    def run_epoch(self , sess , input_data , input_label):\n","        epoch_cost = 0\n","        minibatches = self.get_random_mini_batch()\n","\n","        for minibatch in minibatches:\n","            mini_X , mini_Y = minibatch\n","\n","            _ , temp_cost = sess.run([self.optimizer , self.cost] , \n","                                     feed_dict = {self.X:mini_X , self.Y:mini_Y})\n","            epoch_cost += temp_cost\n","        \n","        return epoch_cost \n","    \n","    def saveModel(self , sess , curr_validation_acc , epoch):\n","        saver = tf.train.Saver()\n","        #Saving The Model\n","        if(curr_validation_acc > self.best_validation):\n","            self.best_validation = curr_validation_acc\n","\n","            self.last_improvement_iter = epoch\n","            \n","            save_path = \"{}{}-{}-{}%/{}%\".format(self.checkpoint_save_path ,\n","                                                  \"my-model\",\n","                                                  epoch ,\n","                                                  round(curr_validation_acc*100,2),\n","                                                  round(curr_validation_acc*100,3))\n","#             self.last_saved_model = save_path\n","            with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"w\") as handle:\n","                handle.write(save_path)\n","            \n","            saver.save(sess , save_path)\n","            \n","            self.improved_str = \"*\"\n","        else:\n","            self.improved_str = ''\n","        \n","    \n","    def fit(self):\n","        tf.reset_default_graph()\n","        \n","        self.m , self.img_height , self.img_width , self.img_channel = self.X_train.shape\n","        self.totalClasses = self.Y_train.shape[1]\n","        \n","        with tf.name_scope(\"initial_steps\"):\n","            self.X,self.Y = self.__create_placeholders()\n","            self.__initialize_parameters()\n","        \n","        with tf.name_scope(\"forward_train\"):\n","            Z = self.__add_forward_model(self.X)\n","        \n","        with tf.name_scope(\"cost_op\"):\n","            self.cost = self.add_cost_op(Z ,self.Y)\n","        \n","        with tf.name_scope(\"train\"):\n","            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n","        \n","        with tf.name_scope(\"accuracy\"):\n","            correct_prediction = tf.equal(tf.argmax(Z,1) , tf.argmax(self.Y,1))\n","            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\"))\n","            \n","        with tf.name_scope(\"summarie_Training\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_train = tf.summary.merge_all()\n","            \n","        with tf.name_scope(\"summarie_Dev\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_dev = tf.summary.merge_all()\n","            \n","        with tf.name_scope(\"summarie_Test\"):\n","            tf.summary.scalar(\"cost\" , self.cost)\n","            tf.summary.scalar(\"acc\" , accuracy)\n","            self.summary_test = tf.summary.merge_all()\n","            \n","        writer  = tf.summary.FileWriter(\"../tensorboard/./graphs/{}\".format(tensorFLowCnnModel.graph_summary_number))\n","        tensorFLowCnnModel.graph_summary_number += 1\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            \n","            writer.add_graph(sess.graph)\n","            \n","            costs = []\n","            \n","            #Restore Model If Exixts\n","            try:\n","                print(\"{}last_model.txt\".format(self.checkpoint_save_path))\n","                with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"r\") as file:\n","                    last_saved_model = file.read()\n","                    \n","                saver = tf.train.Saver()\n","                saver.restore(sess , last_saved_model)\n","            except:\n","                print(\"No Parameters Saved Yet\")\n","            \n","            for epoch in range(self.num_epochs):\n","                epoch_cost  = self.run_epoch(sess , self.X_train , self.Y_train)\n","\n","                curr_validation_acc = sess.run(accuracy , {self.X:self.X_dev , self.Y:self.Y_dev}) \n","                \n","                \n","            \n","                runned_summary = sess.run(self.summary_train , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                runned_summary = sess.run(self.summary_dev , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                runned_summary = sess.run(self.summary_test , {self.X:self.X_train , self.Y:self.Y_train})\n","                writer.add_summary(runned_summary , epoch)\n","                \n","                self.saveModel(sess , curr_validation_acc , epoch)\n","                \n","                print(\"Cost after Epoch %i: %f\" % (epoch + 1 , epoch_cost),\n","                      \"=> Training Acc : %f\" %(sess.run(accuracy , {self.X:self.X_train , self.Y:self.Y_train})),\n","                      \"=> Dev Acc : %f\" %(curr_validation_acc),\n","                      \"=> Test Acc : %f\" %(sess.run(accuracy , {self.X:self.X_test , self.Y:self.Y_test})),\n","                      self.improved_str\n","                     )\n","                \n","                \n","                costs.append(epoch_cost)\n","#                 writer.add_summary(self.summary)\n","        writer.close()\n","        plt.plot(range(1,self.num_epochs+1) , costs)\n","        plt.ylabel('cost')\n","        plt.xlabel('iteration')\n","        plt.show()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8DoJo3jFjoN_","colab_type":"text"},"source":["# **Keras Model**"]},{"cell_type":"code","metadata":{"id":"Uh3BC6UGjmUk","colab_type":"code","colab":{}},"source":["class kerasModel():\n","\n","  def create_model():\n","    raise NotImplementedError(\"create_model is not Implemented Yet!\")\n","    \n","  def compile_model():\n","    raise NotImplementedError(\"compile_model is not Implemeted Yet!\")\n","    \n","  def run_model():\n","    raise NotImplementedError(\"run_model is not Implemented Yet!\")\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H9xry1C-mYmc","colab_type":"code","colab":{}},"source":["class KerasCnn(kerasModel):\n","  def __createConvLayers():\n","    pass\n"," \n","  def __createDenseLayers():\n","    pass\n","  \n","  def create_callbacks():\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5HXA108gKFD","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","#Callback Class 1\n","class onBeginCallback(tf.keras.callbacks.Callback):\n","  #logs contains batch_num and batch_size\n","  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n","    self.on_trainBegin = on_train\n","    self.on_testBegin = on_test\n","    self.on_epochBegin = on_epoch\n","    \n","  def on_train_batch_begin(self,batch , logs=None):\n","    if(self.on_trainBegin):\n","      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_test_batch_begin(self,batch , logs=None):\n","    if(self.on_testBegin):\n","      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n","  \n","  def on_epoch_begin(self, epoch, logs=None):\n","    if(self.on_epochBegin):\n","      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n","  \n","\n","#Callback Class 2\n","class onEndCallback(tf.keras.callbacks.Callback):\n","  #logs contains batch_num , batch_size , loss and Matric\n","  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n","    self.on_trainEnd = on_train\n","    self.on_testEnd = on_test\n","    self.on_epochEnd = on_epoch\n","  def on_train_batch_end(self,batch , logs=None):\n","    if(self.on_trainEnd):\n","      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_test_batch_end(self,batch , logs=None):\n","    if(self.on_testEnd):\n","      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n","    \n","  def on_epoch_end(self, epoch, logs=None):\n","    if(self.on_epochEnd):\n","      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WT3CWtO6tFQR","colab_type":"code","colab":{}},"source":["class kerasCnnModel(KerasCnn):\n","  def __init__(self, parametersDict , callbackDict):\n","    self.callbackDict = callbackDict\n","    self.parametersDict = parametersDict\n","    self.filters = parametersDict['filters']\n","    self.Conv_layers = parametersDict['layers'][0]\n","    self.Dense_layers = parametersDict['layers'][1]\n","    self.kernel_size = parametersDict['kernel_size']\n","    self.padding = parametersDict['padding']\n","    self.dense_layers_size = parametersDict['dense_layers_size']\n","    self.activation = parametersDict['activation']\n","    self.pool_size = parametersDict['pool_size']\n","    \n","    ###RUN INITIAL FUNTIONS\n","    self.__create_callbacks()\n","    \n","  def __createConvLayers(self):\n","    for i in range(self.Conv_layers):\n","      if i != 0:\n","        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , padding = self.padding[i][0]))\n","      else:\n","        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , input_shape = X_train.shape[1:]))\n","\n","      self.model.add(Activation(self.activation[i]))\n","      self.model.add(MaxPooling2D(pool_size = (self.pool_size[i] , self.pool_size[i]) , padding = self.padding[i][1]))\n","      \n","      \n","  def __createDenseLayers(self):\n","    for i in range(self.Dense_layers):\n","      self.model.add(Dense(self.dense_layers_size[i]))\n","      self.model.add(Activation(self.activation[self.Conv_layers + i]))\n","      \n","  \n","  def create_model(self):\n","    self.model = Sequential()\n","    self.__createConvLayers()\n","    self.model.add(Flatten())\n","    self.__createDenseLayers()\n","    \n","    return self.model\n","  \n","  \n","  def compile_model(self , loss=\"sparse_categorical_crossentropy\"  , optimizer=\"adam\" , metrics=['accuracy']):\n","    self.model.compile(loss = \"sparse_categorical_crossentropy\",\n","                       optimizer = \"adam\",\n","                       metrics = ['accuracy']\n","                      )\n","  \n","  def run_model(self , X , Y , X_dev , Y_dev , epochs=5 , batch_size=64 ):\n","    history_callback = self.model.fit( X \n","                   ,Y \n","                  ,epochs=epochs\n","                  ,batch_size=batch_size\n","                  ,validation_data = (X_dev , Y_dev)\n","                  ,callbacks=self.callbacks)\n","  \n","  \n","  def evaluate_model(self , x_test , y_test):\n","    self.model.evaluate(x_test , y_test)\n","    \n","    \n","  def __create_tensorboard_callback(self):\n","    return TensorBoard(log_dir=self.log_dir)\n","  \n","  \n","  def __create_ModelCheckpoint(self):\n","    path = self.callbackDict['modelcheckpoint_path'] + 'model.{epoch:02d}-{val_loss:.2f}.hdf5'\n","    return ModelCheckpoint(filepath=path\n","                          ,monitor= 'val_loss'\n","                          ,verbose= 0\n","                          ,save_best_only=True\n","                          ,mode=min\n","                          ,save_weights_only=False\n","                          ,period=1\n","                          )\n","  \n","  \n","  def __create_callbacks(self , log_dir=\"/content/gdrive/My Drive/SeedApp/data/logs\"):\n","    self.log_dir = log_dir\n","    self.callbacks = []\n","    \n","    if(callbackDict['custom']):\n","      self.callbacks.append(onBeginCallback())\n","      self.callbacks.append(onEndCallback())\n","      \n","    if(callbackDict['tensorboard']):\n","      cb = self.__create_tensorboard_callback()\n","      self.callbacks.append(cb)\n","      \n","    if(callbackDict['modelcheckpoint']):\n","      cb = self.__create_ModelCheckpoint()\n","      self.callbacks.append(cb)\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yW1O95sMCMTK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}