{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qk9bYVofkZl3"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T14:04:50.158262Z",
     "start_time": "2019-05-24T14:04:50.112358Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "I1PBHo1RjmUA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9537,
     "status": "ok",
     "timestamp": 1560325479933,
     "user": {
      "displayName": "ubaid manzoor wani",
      "photoUrl": "https://lh6.googleusercontent.com/-ItGmv7lvkd0/AAAAAAAAAAI/AAAAAAAAAXk/1wbuvLTP0Ck/s64/photo.jpg",
      "userId": "15465009476459618933"
     },
     "user_tz": -330
    },
    "id": "TDhfXiWOnQvo",
    "outputId": "20bbde82-8bfb-4596-a17c-6e88659c555a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model.add` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_DataModule():\n",
    "  DataModule = drive.CreateFile({'id':'1OQiv4PxLTDKGUrsq15KF6VyHqyMq58mi'})\n",
    "  DataModule.GetContentFile('DataUtilsModule.ipynb')\n",
    "  %run DataUtilsModule.ipynb\n",
    "\n",
    "load_DataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00ZR8MOmjipD"
   },
   "source": [
    "# **TensorFlow Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hW6GNL9UQcYT"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:55.795543Z",
     "start_time": "2019-06-14T09:39:55.779760Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "euW4gaJrjmUR"
   },
   "outputs": [],
   "source": [
    "class TenserFlowModel(object):\n",
    "    \n",
    "    def load_data(self):\n",
    "        raise NotImplementedError(\"Load_data() Need To be Implemented\")\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        raise NotImplementedError(\"create_placeholders() Needs to be Implemented\")\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        raise NotImplementedError(\"initialize_parameters() Needs to be Implemented\")\n",
    "        \n",
    "    def create_feed_dict(self , input_batch , label_batch):\n",
    "        raise NotImplementedError(\"create_feed_dict() Needs to be Implemented\")\n",
    "        \n",
    "    def add_forward_model(self , input_data):\n",
    "        raise NotImplementedError(\"add_model() Needs to be Implemented\")\n",
    "        \n",
    "    def add_cost_op(self , predict):\n",
    "        raise NotImplementedError(\"add_loss_op() Needs to be Implemented\")\n",
    "    \n",
    "    def run_epoch(self , sess , input_data , input_labels):\n",
    "        raise NotImplementedError(\"run_epoch Needs to be Implemented\")\n",
    "    \n",
    "    def fit(self):\n",
    "        raise NotImplementedError(\"fit() Needs to be Implemented\")\n",
    "        \n",
    "    def predict(self , sess , input_data , input_labels=None):\n",
    "        raise NotImplementedError(\"predict() Needs to be Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:57.427359Z",
     "start_time": "2019-06-14T09:39:57.333762Z"
    },
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "i_jRklBKjmUa"
   },
   "outputs": [],
   "source": [
    "class tensorFLowCnnModel(TenserFlowModel):\n",
    "    def __init__(self):\n",
    "        tf.set_random_seed(1)\n",
    "        self.img_height = None\n",
    "        self.img_width = None\n",
    "        self.img_channel = None\n",
    "        self.totalClasses = 5\n",
    "        self.learning_parameter_shape = {}\n",
    "        self.learning_parameters = {}\n",
    "        self.non_learning_parameters = {}\n",
    "        self.stride = None\n",
    "        self.window_size = None\n",
    "        self.gray_scale = False\n",
    "        self.dropout = []\n",
    "        self.fully_connented_dims = []\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs=10\n",
    "        self.dataPath = '../data/working/'\n",
    "        self.minibatch_size = 64\n",
    "        self.m = None\n",
    "        self.best_validation = 0.0\n",
    "        self.checkpoint_save_path = '../data/CheckPoints/'\n",
    "        self.last_improvement_iter = None\n",
    "        self.improved_str = ''\n",
    "#         self.last_saved_model = None\n",
    "        cnn.graph_summary_number = 1\n",
    "        #Funtion\n",
    "        self.load_data()\n",
    "        self.apply_one_hot()\n",
    "        self.__set_shape_var_of_Eachlayers()\n",
    "        \n",
    "        \n",
    "    def apply_one_hot(self):\n",
    "        self.Y_train = tf.one_hot(self.Y_train , self.totalClasses)\n",
    "        self.Y_test = tf.one_hot(self.Y_test , self.totalClasses)\n",
    "        self.Y_dev = tf.one_hot(self.Y_dev , self.totalClasses)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self.Y_train , self.Y_test,self.Y_dev = sess.run([self.Y_train , self.Y_test , self.Y_dev]) \n",
    "#         print(\"After OneHot\")\n",
    "#         print(self.X_train.shape)\n",
    "#         print(self.Y_train.shape)\n",
    "\n",
    "#         print(self.X_test.shape)\n",
    "#         print(self.Y_test.shape)\n",
    "\n",
    "#         print(self.X_dev.shape)\n",
    "#         print(self.Y_dev.shape)\n",
    "            \n",
    "    def load_data(self):\n",
    "        #if Data is Already is stored\n",
    "        if(utils.ifDataExits('X_Train' , self.dataPath)):\n",
    "            self.X_train , self.X_test , self.X_dev , self.Y_train , self.Y_test , self.Y_dev \\\n",
    "                                                                        = utils.load_saved_data()\n",
    "#             print(self.X_train.shape)\n",
    "#             print(self.Y_train.shape)\n",
    "\n",
    "#             print(self.X_test.shape)\n",
    "#             print(self.Y_test.shape)\n",
    "\n",
    "#             print(self.X_dev.shape)\n",
    "#             print(self.Y_dev.shape)\n",
    "        else:\n",
    "            self.X_train,self.X_test,self.Y_train,self.Y_test,self.X_dev,self.Y_dev  \\\n",
    "                                    = utils.load_data(5 ,img_size=100 , batch_size=400) \n",
    "    def __set_shape_var_of_Eachlayers(self):\n",
    "        #For Learning Parameters\n",
    "        filter_shape = [\n",
    "                            [4,4,1,8] ,  #Layer 1\n",
    "                            [4,4,8,16] , #Layer 2\n",
    "                            [4,4,16,32] ,#Layer 3 \n",
    "                            [4,4,32,32] ,#Layer 4\n",
    "                            [4,4,32,64]  #Layer 5\n",
    "                        ]\n",
    "        if not self.gray_scale:\n",
    "            filter_shape[0][2] = 3 \n",
    "        \n",
    "        for index,shape in enumerate(filter_shape):\n",
    "            layer = index+1\n",
    "            self.learning_parameter_shape[\"W\" + str(layer)] = shape\n",
    "#         print(\"Shape :\" , self.learning_parameter_shape)\n",
    "        #For Non-Learning Parameters\n",
    "        \n",
    "        #[for first maxpool layer,for second,for third , ...]\n",
    "        self.window_size = [2,2,2,2,2]\n",
    "        #[stride of conv layer , stride for max pool layer]\n",
    "        self.stride = [[1,2] , [1,2] , [1,2] , [1,2] , [1,2]]\n",
    "        self.dropout = [1 , 1 ,0.7 , 1 , 1]\n",
    "        \n",
    "        #Shape For FullyConnected Layer\n",
    "        self.fully_connented_dims = [128 , 64 , 5] #for last Layer except sofmax Layer\n",
    "        \n",
    "    def __create_placeholders(self):\n",
    "        X = tf.placeholder(dtype=tf.float32 , \n",
    "                           shape=[None , self.img_height , self.img_width , self.img_channel] ,\n",
    "                           name = 'input_values'\n",
    "                          )\n",
    "        Y = tf.placeholder(dtype=tf.float32 ,\n",
    "                           shape=[None , self.totalClasses] ,\n",
    "                           name=\"input_labels\"\n",
    "                          )\n",
    "        return X,Y\n",
    "    \n",
    "    def __initialize_learning_parameters(self):\n",
    "        layers = len(self.learning_parameter_shape)\n",
    "        for l in range(1, layers+1):\n",
    "            self.learning_parameters[\"W\"+str(l)]= \\\n",
    "                                    tf.get_variable(\"W\"+str(l) ,\n",
    "                                                    self.learning_parameter_shape[\"W\"+str(l)] ,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer(seed=1)\n",
    "                                                   )\n",
    "#         print(\"Parameters\" , self.learning_parameters)\n",
    "    def __initialize_non_learning_parameters(self):\n",
    "        layers = len(self.stride)\n",
    "        \n",
    "        for l in range(1,layers + 1):\n",
    "            self.non_learning_parameters[\"Conv_s\"+str(l)] = self.stride[l-1][0]\n",
    "            self.non_learning_parameters[\"Maxpool_s\"+str(l)] = self.stride[l-1][1]\n",
    "            self.non_learning_parameters[\"Window_size\"+str(l)] = self.window_size[l-1]\n",
    "            self.non_learning_parameters[\"dropout\"+str(l)] = self.dropout[l-1]\n",
    "            \n",
    "        \n",
    "    def __initialize_parameters(self):\n",
    "        self.__set_shape_var_of_Eachlayers()\n",
    "        \n",
    "        self.__initialize_learning_parameters()\n",
    "        self.__initialize_non_learning_parameters()\n",
    "\n",
    "        \n",
    "    def __create_feed_dict(self , input_batch , label_batch):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.X] = input_batch\n",
    "        feed_dict[self.Y] = label_batch\n",
    "        \n",
    "        return feed_dict\n",
    "    \n",
    "    def layersBeforeFlatten(self, X):\n",
    "        LP = self.learning_parameters\n",
    "        NLP = self.non_learning_parameters\n",
    "        layers = len(LP)\n",
    "        \n",
    "        for l in range(1,layers+1):\n",
    "            #Extract All parameters Needed\n",
    "            conv_s = NLP[\"Conv_s\"+str(l)]\n",
    "            f = NLP[\"Window_size\"+str(l)]\n",
    "            pool_s = NLP[\"Maxpool_s\"+str(l)]\n",
    "            curr_filter = LP[\"W\"+str(l)]\n",
    "#             print(\"curr_filter\" , curr_filter)\n",
    "                \n",
    "            #CONV Layer\n",
    "            s = conv_s\n",
    "            Z = tf.nn.conv2d(X , \n",
    "                             curr_filter , \n",
    "                             strides=[1,s,s,1] , \n",
    "                             padding='SAME' , \n",
    "                             name=\"Conv_Layer_{}\".format(l))\n",
    "            \n",
    "            #Activation\n",
    "            A = tf.nn.relu(Z , \n",
    "                           name=\"Relu_{}\".format(l))\n",
    "            \n",
    "            #Maxpool\n",
    "            s = pool_s\n",
    "            features = tf.nn.max_pool(A , \n",
    "                                      ksize=[1,f,f,1] , \n",
    "                                      strides=[1,s,s,1] ,\n",
    "                                      padding='SAME' , \n",
    "                                      name=\"Max_pool_{}\".format(l))\n",
    "            features = tf.nn.dropout(features ,\n",
    "                                     rate=1-self.dropout[l-1])\n",
    "            X = features\n",
    "        return features\n",
    "\n",
    "    def LayersAfterFlatten(self , X):\n",
    "        layers = len(self.fully_connented_dims)\n",
    "        \n",
    "        for l in range(1,layers+1):\n",
    "            Z = tf.contrib.layers.fully_connected(X  , self.fully_connented_dims[l-1] , activation_fn=None)\n",
    "        return Z\n",
    "    \n",
    "    def __add_forward_model(self , input_data):\n",
    "        X = self.layersBeforeFlatten(input_data)\n",
    "        \n",
    "        X_flatten = tf.contrib.layers.flatten(X)\n",
    "        \n",
    "        Z = self.LayersAfterFlatten(X_flatten)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def add_cost_op(self , predict , Y):\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict , labels=Y))\n",
    "        return cost\n",
    "    \n",
    "    def get_random_mini_batch(self ,mini_batch_size = 64):\n",
    "        mini_batches = []\n",
    "        \n",
    "        #About Random actually data is already Shuffled\n",
    "\n",
    "        num_compelete_minibatches = math.floor(self.m / mini_batch_size)\n",
    "        for k in range(num_compelete_minibatches):\n",
    "            mini_batch_X = self.X_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "            mini_batch_Y = self.Y_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        if self.m%num_compelete_minibatches != 0:\n",
    "            mini_batch_X = self.X_train[num_compelete_minibatches * mini_batch_size : self.m,:,:,:]\n",
    "            mini_batch_Y = self.Y_train[num_compelete_minibatches * mini_batch_size : self.m,:]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches\n",
    "    \n",
    "    def run_epoch(self , sess , input_data , input_label):\n",
    "        epoch_cost = 0\n",
    "        minibatches = self.get_random_mini_batch()\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            mini_X , mini_Y = minibatch\n",
    "\n",
    "            _ , temp_cost = sess.run([self.optimizer , self.cost] , \n",
    "                                     feed_dict = {self.X:mini_X , self.Y:mini_Y})\n",
    "            epoch_cost += temp_cost\n",
    "        \n",
    "        return epoch_cost \n",
    "    \n",
    "    def saveModel(self , sess , curr_validation_acc , epoch):\n",
    "        saver = tf.train.Saver()\n",
    "        #Saving The Model\n",
    "        if(curr_validation_acc > self.best_validation):\n",
    "            self.best_validation = curr_validation_acc\n",
    "\n",
    "            self.last_improvement_iter = epoch\n",
    "            \n",
    "            save_path = \"{}{}-{}-{}%/{}%\".format(self.checkpoint_save_path ,\n",
    "                                                  \"my-model\",\n",
    "                                                  epoch ,\n",
    "                                                  round(curr_validation_acc*100,2),\n",
    "                                                  round(curr_validation_acc*100,3))\n",
    "#             self.last_saved_model = save_path\n",
    "            with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"w\") as handle:\n",
    "                handle.write(save_path)\n",
    "            \n",
    "            saver.save(sess , save_path)\n",
    "            \n",
    "            self.improved_str = \"*\"\n",
    "        else:\n",
    "            self.improved_str = ''\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.m , self.img_height , self.img_width , self.img_channel = self.X_train.shape\n",
    "        self.totalClasses = self.Y_train.shape[1]\n",
    "        \n",
    "        with tf.name_scope(\"initial_steps\"):\n",
    "            self.X,self.Y = self.__create_placeholders()\n",
    "            self.__initialize_parameters()\n",
    "        \n",
    "        with tf.name_scope(\"forward_train\"):\n",
    "            Z = self.__add_forward_model(self.X)\n",
    "        \n",
    "        with tf.name_scope(\"cost_op\"):\n",
    "            self.cost = self.add_cost_op(Z ,self.Y)\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(Z,1) , tf.argmax(self.Y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\"))\n",
    "            \n",
    "        with tf.name_scope(\"summarie_Training\"):\n",
    "            tf.summary.scalar(\"cost\" , self.cost)\n",
    "            tf.summary.scalar(\"acc\" , accuracy)\n",
    "            self.summary_train = tf.summary.merge_all()\n",
    "            \n",
    "        with tf.name_scope(\"summarie_Dev\"):\n",
    "            tf.summary.scalar(\"cost\" , self.cost)\n",
    "            tf.summary.scalar(\"acc\" , accuracy)\n",
    "            self.summary_dev = tf.summary.merge_all()\n",
    "            \n",
    "        with tf.name_scope(\"summarie_Test\"):\n",
    "            tf.summary.scalar(\"cost\" , self.cost)\n",
    "            tf.summary.scalar(\"acc\" , accuracy)\n",
    "            self.summary_test = tf.summary.merge_all()\n",
    "            \n",
    "        writer  = tf.summary.FileWriter(\"../tensorboard/./graphs/{}\".format(cnn.graph_summary_number))\n",
    "        cnn.graph_summary_number += 1\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            writer.add_graph(sess.graph)\n",
    "            \n",
    "            costs = []\n",
    "            \n",
    "            #Restore Model If Exixts\n",
    "            try:\n",
    "                print(\"{}last_model.txt\".format(self.checkpoint_save_path))\n",
    "                with open(\"{}last_model.txt\".format(self.checkpoint_save_path) , \"r\") as file:\n",
    "                    last_saved_model = file.read()\n",
    "                    \n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess , last_saved_model)\n",
    "            except:\n",
    "                print(\"No Parameters Saved Yet\")\n",
    "            \n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_cost  = self.run_epoch(sess , self.X_train , self.Y_train)\n",
    "\n",
    "                curr_validation_acc = sess.run(accuracy , {self.X:self.X_dev , self.Y:self.Y_dev}) \n",
    "                \n",
    "                \n",
    "            \n",
    "                runned_summary = sess.run(self.summary_train , {self.X:self.X_train , self.Y:self.Y_train})\n",
    "                writer.add_summary(runned_summary , epoch)\n",
    "                runned_summary = sess.run(self.summary_dev , {self.X:self.X_train , self.Y:self.Y_train})\n",
    "                writer.add_summary(runned_summary , epoch)\n",
    "                runned_summary = sess.run(self.summary_test , {self.X:self.X_train , self.Y:self.Y_train})\n",
    "                writer.add_summary(runned_summary , epoch)\n",
    "                \n",
    "                self.saveModel(sess , curr_validation_acc , epoch)\n",
    "                \n",
    "                print(\"Cost after Epoch %i: %f\" % (epoch + 1 , epoch_cost),\n",
    "                      \"=> Training Acc : %f\" %(sess.run(accuracy , {self.X:self.X_train , self.Y:self.Y_train})),\n",
    "                      \"=> Dev Acc : %f\" %(curr_validation_acc),\n",
    "                      \"=> Test Acc : %f\" %(sess.run(accuracy , {self.X:self.X_test , self.Y:self.Y_test})),\n",
    "                      self.improved_str\n",
    "                     )\n",
    "                \n",
    "                \n",
    "                costs.append(epoch_cost)\n",
    "#                 writer.add_summary(self.summary)\n",
    "        writer.close()\n",
    "        plt.plot(range(1,self.num_epochs+1) , costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iteration')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DoJo3jFjoN_"
   },
   "source": [
    "# **Keras Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:57.612266Z",
     "start_time": "2019-06-14T09:39:57.607391Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Uh3BC6UGjmUk"
   },
   "outputs": [],
   "source": [
    "class kerasModel():\n",
    "\n",
    "  def create_model():\n",
    "    raise NotImplementedError(\"create_model is not Implemented Yet!\")\n",
    "    \n",
    "  def compile_model():\n",
    "    raise NotImplementedError(\"compile_model is not Implemeted Yet!\")\n",
    "    \n",
    "  def run_model():\n",
    "    raise NotImplementedError(\"run_model is not Implemented Yet!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:57.772980Z",
     "start_time": "2019-06-14T09:39:57.767954Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "H9xry1C-mYmc"
   },
   "outputs": [],
   "source": [
    "class KerasCnn(kerasModel):\n",
    "  def __createConvLayers():\n",
    "    pass\n",
    " \n",
    "  def __createDenseLayers():\n",
    "    pass\n",
    "  \n",
    "  def create_callbacks():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:57.943675Z",
     "start_time": "2019-06-14T09:39:57.922665Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Callback Class 1\n",
    "class onBeginCallback(tf.keras.callbacks.Callback):\n",
    "  #logs contains batch_num and batch_size\n",
    "  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n",
    "    self.on_trainBegin = on_train\n",
    "    self.on_testBegin = on_test\n",
    "    self.on_epochBegin = on_epoch\n",
    "    \n",
    "  def on_train_batch_begin(self,batch , logs=None):\n",
    "    if(self.on_trainBegin):\n",
    "      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n",
    "    \n",
    "  def on_test_batch_begin(self,batch , logs=None):\n",
    "    if(self.on_testBegin):\n",
    "      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n",
    "  \n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    if(self.on_epochBegin):\n",
    "      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n",
    "  \n",
    "\n",
    "#Callback Class 2\n",
    "class onEndCallback(tf.keras.callbacks.Callback):\n",
    "  #logs contains batch_num , batch_size , loss and Matric\n",
    "  def __init__(self , on_train = False , on_test = False , on_epoch=False):\n",
    "    self.on_trainEnd = on_train\n",
    "    self.on_testEnd = on_test\n",
    "    self.on_epochEnd = on_epoch\n",
    "  def on_train_batch_end(self,batch , logs=None):\n",
    "    if(self.on_trainEnd):\n",
    "      print(\"\\non_train_batch_begin->Logs: {}\\n\".format(logs))\n",
    "    \n",
    "  def on_test_batch_end(self,batch , logs=None):\n",
    "    if(self.on_testEnd):\n",
    "      print(\"\\non_test_batch_begin->Logs: {}\\n\".format(logs))\n",
    "    \n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if(self.on_epochEnd):\n",
    "      print(\"\\non_epoch_begin->Logs: {}\\n\".format(logs))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T09:39:58.139414Z",
     "start_time": "2019-06-14T09:39:58.112503Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WT3CWtO6tFQR"
   },
   "outputs": [],
   "source": [
    "class kerasCnnModel(KerasCnn):\n",
    "  def __init__(self, parametersDict , callbackDict):\n",
    "    self.callbackDict = callbackDict\n",
    "    self.parametersDict = parametersDict\n",
    "    self.filters = parametersDict['filters']\n",
    "    self.Conv_layers = parametersDict['layers'][0]\n",
    "    self.Dense_layers = parametersDict['layers'][1]\n",
    "    self.kernel_size = parametersDict['kernel_size']\n",
    "    self.padding = parametersDict['padding']\n",
    "    self.dense_layers_size = parametersDict['dense_layers_size']\n",
    "    self.activation = parametersDict['activation']\n",
    "    self.pool_size = parametersDict['pool_size']\n",
    "    \n",
    "    ###RUN INITIAL FUNTIONS\n",
    "    self.__create_callbacks()\n",
    "    \n",
    "  def __createConvLayers(self):\n",
    "    for i in range(self.Conv_layers):\n",
    "      if i != 0:\n",
    "        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , padding = self.padding[i][0]))\n",
    "      else:\n",
    "        self.model.add(Conv2D(filters = self.filters[i], kernel_size = (self.kernel_size[i],self.kernel_size[i]) , input_shape = X_train.shape[1:]))\n",
    "\n",
    "      self.model.add(Activation(self.activation[i]))\n",
    "      self.model.add(MaxPooling2D(pool_size = (self.pool_size[i] , self.pool_size[i]) , padding = self.padding[i][1]))\n",
    "      \n",
    "      \n",
    "  def __createDenseLayers(self):\n",
    "    for i in range(self.Dense_layers):\n",
    "      self.model.add(Dense(self.dense_layers_size[i]))\n",
    "      self.model.add(Activation(self.activation[self.Conv_layers + i]))\n",
    "      \n",
    "  \n",
    "  def create_model(self):\n",
    "    self.model = Sequential()\n",
    "    self.__createConvLayers()\n",
    "    self.model.add(Flatten())\n",
    "    self.__createDenseLayers()\n",
    "    \n",
    "    return self.model\n",
    "  \n",
    "  \n",
    "  def compile_model(self , loss=\"sparse_categorical_crossentropy\"  , optimizer=\"adam\" , metrics=['accuracy']):\n",
    "    self.model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                       optimizer = \"adam\",\n",
    "                       metrics = ['accuracy']\n",
    "                      )\n",
    "  \n",
    "  def run_model(self , X , Y , X_dev , Y_dev , epochs=5 , batch_size=64 ):\n",
    "    history_callback = self.model.fit( X \n",
    "                   ,Y \n",
    "                  ,epochs=epochs\n",
    "                  ,batch_size=batch_size\n",
    "                  ,validation_data = (X_dev , Y_dev)\n",
    "                  ,callbacks=self.callbacks)\n",
    "  \n",
    "  \n",
    "  def evaluate_model(self , x_test , y_test):\n",
    "    self.model.evaluate(x_test , y_test)\n",
    "    \n",
    "    \n",
    "  def __create_tensorboard_callback(self):\n",
    "    return TensorBoard(log_dir=self.log_dir)\n",
    "  \n",
    "  \n",
    "  def __create_ModelCheckpoint(self):\n",
    "    path = self.callbackDict['modelcheckpoint_path'] + 'model.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "    return ModelCheckpoint(filepath=path\n",
    "                          ,monitor= 'val_loss'\n",
    "                          ,verbose= 0\n",
    "                          ,save_best_only=True\n",
    "                          ,mode=min\n",
    "                          ,save_weights_only=False\n",
    "                          ,period=1\n",
    "                          )\n",
    "  \n",
    "  \n",
    "  def __create_callbacks(self , log_dir=\"/content/gdrive/My Drive/SeedApp/data/logs\"):\n",
    "    self.log_dir = log_dir\n",
    "    self.callbacks = []\n",
    "    \n",
    "    if(callbackDict['custom']):\n",
    "      self.callbacks.append(onBeginCallback())\n",
    "      self.callbacks.append(onEndCallback())\n",
    "      \n",
    "    if(callbackDict['tensorboard']):\n",
    "      cb = self.__create_tensorboard_callback()\n",
    "      self.callbacks.append(cb)\n",
    "      \n",
    "    if(callbackDict['modelcheckpoint']):\n",
    "      cb = self.__create_ModelCheckpoint()\n",
    "      self.callbacks.append(cb)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yW1O95sMCMTK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "models.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
